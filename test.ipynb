{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "\n",
    "# tf.config.experimental.set_visible_devices(devices=cpus[0], device_type='CPU')\n",
    "# tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(device=gpu, enable=True)\n",
    "    \n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_parser.config import CONFIG\n",
    "from model.models import CNN, MLP\n",
    "from model.layers import Maxout, InceptionModule, AnonymousNetBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_list():\n",
    "    num_classes = 257\n",
    "    label_to_name = [None for i in range(num_classes)]\n",
    "    base_dir = CONFIG['train'].data_dir\n",
    "    dir_names = os.listdir(base_dir)\n",
    "    for dir_name in dir_names:\n",
    "        if not os.path.isdir(os.path.join(base_dir, dir_name)):\n",
    "            continue\n",
    "        label, name = dir_name.split('.')\n",
    "        label_to_name[int(label)-1] = name\n",
    "    assert all(label_to_name)\n",
    "    return label_to_name\n",
    "    \n",
    "def load_data(data_type='train'):\n",
    "\n",
    "    assert data_type in ['train', 'test']\n",
    "    config = CONFIG[data_type]\n",
    "\n",
    "    csv=pd.read_csv(config.data_labels_file)\n",
    "    img_paths = []\n",
    "    for name in csv[:]['Name'].values:\n",
    "        name = name.strip(\"'\").replace('\\\\', '/')\n",
    "        img_path = os.path.normpath(os.path.join(config.data_dir, name))\n",
    "        img_paths.append(img_path)\n",
    "    img_paths = np.array(img_paths)\n",
    "\n",
    "    labels = csv[:]['Label'].values - 1\n",
    "\n",
    "    return img_paths, labels\n",
    "\n",
    "def load_data_from_path(img_paths, shape='random', part='random'):\n",
    "    size = min(CONFIG.image.W, CONFIG.image.H)\n",
    "    size = int(size * (np.random.random()/2 + 1))\n",
    "    if shape == 'random':\n",
    "        shape = (size, size)\n",
    "    data_size = len(img_paths)\n",
    "    data = np.zeros((data_size, shape[1], shape[0], 3), dtype=np.uint8)\n",
    "    for i, img_path in enumerate(img_paths):\n",
    "        print(\"{}/{} load {} -> {}\".format(i, data_size, img_path, shape), end='\\r')\n",
    "        if part == 'random':\n",
    "            part = -1 if np.random.random() < 0.5 else np.random.random()\n",
    "        data[i] = read_and_resize(img_path, shape=shape, part=part)\n",
    "#         plt.imshow(data[i])\n",
    "#         plt.show()\n",
    "    return data\n",
    "\n",
    "def read_and_resize(img_path, shape=(96, 96), part=-1):\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    if part != -1:\n",
    "        h, w = img.shape[: 2]\n",
    "        start = int(part * abs(w-h))\n",
    "        if h < w:\n",
    "            img = img[:, start: start+h]\n",
    "        elif w > h:\n",
    "            img = img[start: start+w]\n",
    "    return cv2.resize(img, shape)\n",
    "\n",
    "def batch_loader(X, y, batch_size=64):\n",
    "\n",
    "    data_size = X.shape[0]\n",
    "    permutation = np.random.permutation(data_size)\n",
    "    batch_permutation_indices = [permutation[i: i + batch_size] for i in range(0, data_size, batch_size)]\n",
    "    for batch_permutation in batch_permutation_indices:\n",
    "        yield X[batch_permutation], y[batch_permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        # crop images by -5% to 10% of their height/width\n",
    "        sometimes(iaa.CropAndPad(\n",
    "            percent=(-0.05, 0.1),\n",
    "            pad_mode=ia.ALL,\n",
    "            pad_cval=(0, 255)\n",
    "        )),\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n",
    "            translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, # translate by -20 to +20 percent (per axis)\n",
    "            rotate=(-10, 10), # rotate by -45 to +45 degrees\n",
    "            shear=(-3, 3), # shear by -16 to +16 degrees\n",
    "            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
    "            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        )),\n",
    "        iaa.OneOf([\n",
    "            iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "            iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "            iaa.MedianBlur(k=(3, 7)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "        ]),\n",
    "        sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5)),\n",
    "        iaa.Resize({\"height\":CONFIG.image.H, \"width\":CONFIG.image.W})\n",
    "    ],\n",
    "    random_order=True\n",
    ")\n",
    "\n",
    "resizer = iaa.Sequential(\n",
    "    [\n",
    "        iaa.Resize({\"height\":CONFIG.image.H, \"width\":CONFIG.image.W})\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1000\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "reload_data_step = 5\n",
    "\n",
    "# model = MLP()\n",
    "# model = CNN()\n",
    "# model = tf.keras.models.load_model(CONFIG.model.model_file)\n",
    "# model = tf.keras.applications.MobileNetV2(weights=None, classes=257)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "test_input = np.zeros(shape=(1, CONFIG.image.H, CONFIG.image.W, 3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = tf.keras.applications.DenseNet121(input_shape=(CONFIG.image.H, CONFIG.image.W, 3), weights='imagenet', include_top=False)\n",
    "# base_model.trainable = False\n",
    "# model = tf.keras.Sequential([\n",
    "#     base_model,\n",
    "#     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#     tf.keras.layers.Dense(257),\n",
    "#     tf.keras.layers.Softmax()\n",
    "# ])\n",
    "# model = tf.keras.Sequential([\n",
    "#     base_model,\n",
    "#     tf.keras.layers.SeparableConv2D(filters=512, kernel_size=(1, 1), padding='same'),\n",
    "#     tf.keras.layers.ReLU(),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.SeparableConv2D(filters=257, kernel_size=(7, 7), padding='valid'),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Softmax()\n",
    "# ])\n",
    "# print(model(test_input).shape)\n",
    "# base_model.trainable = True\n",
    "# print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "# fine_tune_at = 300\n",
    "# for layer in base_model.layers[:fine_tune_at]:\n",
    "#     layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "anonymous_net_block (Anonymo (None, 224, 224, 16)      1984      \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_1 (Anony (None, 224, 224, 16)      2608      \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_2 (Anony (None, 112, 112, 32)      12768     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_3 (Anony (None, 112, 112, 32)      9824      \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_4 (Anony (None, 112, 112, 32)      9824      \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_5 (Anony (None, 56, 56, 64)        49088     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_6 (Anony (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_7 (Anony (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_8 (Anony (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_9 (Anony (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_10 (Anon (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_11 (Anon (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_12 (Anon (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_13 (Anon (None, 56, 56, 64)        38080     \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_14 (Anon (None, 28, 28, 128)       192384    \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_15 (Anon (None, 28, 28, 128)       149888    \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_16 (Anon (None, 28, 28, 128)       149888    \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_17 (Anon (None, 14, 14, 256)       761600    \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_18 (Anon (None, 14, 14, 256)       594688    \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_19 (Anon (None, 14, 14, 256)       594688    \n",
      "_________________________________________________________________\n",
      "anonymous_net_block_20 (Anon (None, 7, 7, 512)         3030528   \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 7, 7, 257)         131841    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 257)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 257)               0         \n",
      "=================================================================\n",
      "Total params: 5,996,241\n",
      "Trainable params: 5,991,505\n",
      "Non-trainable params: 4,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = inputs\n",
    "x = AnonymousNetBlock(units=16, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=16, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=32, strides=2, activation=True, batch_norm=True)(x)\n",
    "\n",
    "x = AnonymousNetBlock(units=32, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=32, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=64, strides=2, activation=True, batch_norm=True)(x)\n",
    "\n",
    "for i in range(8):\n",
    "    x = AnonymousNetBlock(units=64, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=128, strides=2, activation=True, batch_norm=True)(x)\n",
    "\n",
    "x = AnonymousNetBlock(units=128, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=128, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=256, strides=2, activation=True, batch_norm=True)(x)\n",
    "\n",
    "x = AnonymousNetBlock(units=256, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=256, strides=1, activation=True, batch_norm=True)(x)\n",
    "x = AnonymousNetBlock(units=512, strides=2, activation=True, batch_norm=True)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=257, kernel_size=[1, 1], padding='same')(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = tf.keras.layers.Softmax()(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26634/26635 load data/TrainSet/062.eiffel-tower/Train_062_0064.jpg -> (224, 224)4))(224, 224)\r"
     ]
    }
   ],
   "source": [
    "X_path, y = load_data(data_type='train')\n",
    "X_path_train, X_path_dev, y_train, y_dev = train_test_split(X_path, y, test_size=0.05, random_state=CONFIG.seed)\n",
    "\n",
    "train_size, dev_size = len(X_path_train), len(X_path_dev)\n",
    "X_dev = load_data_from_path(X_path_dev, shape=(CONFIG.image.W, CONFIG.image.H), part=-1)\n",
    "X_train = load_data_from_path(X_path_train, shape=(CONFIG.image.W, CONFIG.image.H), part=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = get_name_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['anonymous_net_block/batch_normalization/moving_mean:0', 'anonymous_net_block/batch_normalization/moving_variance:0', 'anonymous_net_block_1/batch_normalization_1/moving_mean:0', 'anonymous_net_block_1/batch_normalization_1/moving_variance:0', 'anonymous_net_block_2/batch_normalization_2/moving_mean:0', 'anonymous_net_block_2/batch_normalization_2/moving_variance:0', 'anonymous_net_block_3/batch_normalization_3/moving_mean:0', 'anonymous_net_block_3/batch_normalization_3/moving_variance:0', 'anonymous_net_block_4/batch_normalization_4/moving_mean:0', 'anonymous_net_block_4/batch_normalization_4/moving_variance:0', 'anonymous_net_block_5/batch_normalization_5/moving_mean:0', 'anonymous_net_block_5/batch_normalization_5/moving_variance:0', 'anonymous_net_block_6/batch_normalization_6/moving_mean:0', 'anonymous_net_block_6/batch_normalization_6/moving_variance:0', 'anonymous_net_block_7/batch_normalization_7/moving_mean:0', 'anonymous_net_block_7/batch_normalization_7/moving_variance:0', 'anonymous_net_block_8/batch_normalization_8/moving_mean:0', 'anonymous_net_block_8/batch_normalization_8/moving_variance:0', 'anonymous_net_block_9/batch_normalization_9/moving_mean:0', 'anonymous_net_block_9/batch_normalization_9/moving_variance:0', 'anonymous_net_block_10/batch_normalization_10/moving_mean:0', 'anonymous_net_block_10/batch_normalization_10/moving_variance:0', 'anonymous_net_block_11/batch_normalization_11/moving_mean:0', 'anonymous_net_block_11/batch_normalization_11/moving_variance:0', 'anonymous_net_block_12/batch_normalization_12/moving_mean:0', 'anonymous_net_block_12/batch_normalization_12/moving_variance:0', 'anonymous_net_block_13/batch_normalization_13/moving_mean:0', 'anonymous_net_block_13/batch_normalization_13/moving_variance:0', 'anonymous_net_block_14/batch_normalization_14/moving_mean:0', 'anonymous_net_block_14/batch_normalization_14/moving_variance:0', 'anonymous_net_block_15/batch_normalization_15/moving_mean:0', 'anonymous_net_block_15/batch_normalization_15/moving_variance:0', 'anonymous_net_block_16/batch_normalization_16/moving_mean:0', 'anonymous_net_block_16/batch_normalization_16/moving_variance:0', 'anonymous_net_block_17/batch_normalization_17/moving_mean:0', 'anonymous_net_block_17/batch_normalization_17/moving_variance:0', 'anonymous_net_block_18/batch_normalization_18/moving_mean:0', 'anonymous_net_block_18/batch_normalization_18/moving_variance:0', 'anonymous_net_block_19/batch_normalization_19/moving_mean:0', 'anonymous_net_block_19/batch_normalization_19/moving_variance:0', 'anonymous_net_block_20/batch_normalization_20/moving_mean:0', 'anonymous_net_block_20/batch_normalization_20/moving_variance:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['anonymous_net_block/batch_normalization/moving_mean:0', 'anonymous_net_block/batch_normalization/moving_variance:0', 'anonymous_net_block_1/batch_normalization_1/moving_mean:0', 'anonymous_net_block_1/batch_normalization_1/moving_variance:0', 'anonymous_net_block_2/batch_normalization_2/moving_mean:0', 'anonymous_net_block_2/batch_normalization_2/moving_variance:0', 'anonymous_net_block_3/batch_normalization_3/moving_mean:0', 'anonymous_net_block_3/batch_normalization_3/moving_variance:0', 'anonymous_net_block_4/batch_normalization_4/moving_mean:0', 'anonymous_net_block_4/batch_normalization_4/moving_variance:0', 'anonymous_net_block_5/batch_normalization_5/moving_mean:0', 'anonymous_net_block_5/batch_normalization_5/moving_variance:0', 'anonymous_net_block_6/batch_normalization_6/moving_mean:0', 'anonymous_net_block_6/batch_normalization_6/moving_variance:0', 'anonymous_net_block_7/batch_normalization_7/moving_mean:0', 'anonymous_net_block_7/batch_normalization_7/moving_variance:0', 'anonymous_net_block_8/batch_normalization_8/moving_mean:0', 'anonymous_net_block_8/batch_normalization_8/moving_variance:0', 'anonymous_net_block_9/batch_normalization_9/moving_mean:0', 'anonymous_net_block_9/batch_normalization_9/moving_variance:0', 'anonymous_net_block_10/batch_normalization_10/moving_mean:0', 'anonymous_net_block_10/batch_normalization_10/moving_variance:0', 'anonymous_net_block_11/batch_normalization_11/moving_mean:0', 'anonymous_net_block_11/batch_normalization_11/moving_variance:0', 'anonymous_net_block_12/batch_normalization_12/moving_mean:0', 'anonymous_net_block_12/batch_normalization_12/moving_variance:0', 'anonymous_net_block_13/batch_normalization_13/moving_mean:0', 'anonymous_net_block_13/batch_normalization_13/moving_variance:0', 'anonymous_net_block_14/batch_normalization_14/moving_mean:0', 'anonymous_net_block_14/batch_normalization_14/moving_variance:0', 'anonymous_net_block_15/batch_normalization_15/moving_mean:0', 'anonymous_net_block_15/batch_normalization_15/moving_variance:0', 'anonymous_net_block_16/batch_normalization_16/moving_mean:0', 'anonymous_net_block_16/batch_normalization_16/moving_variance:0', 'anonymous_net_block_17/batch_normalization_17/moving_mean:0', 'anonymous_net_block_17/batch_normalization_17/moving_variance:0', 'anonymous_net_block_18/batch_normalization_18/moving_mean:0', 'anonymous_net_block_18/batch_normalization_18/moving_variance:0', 'anonymous_net_block_19/batch_normalization_19/moving_mean:0', 'anonymous_net_block_19/batch_normalization_19/moving_variance:0', 'anonymous_net_block_20/batch_normalization_20/moving_mean:0', 'anonymous_net_block_20/batch_normalization_20/moving_variance:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['anonymous_net_block/batch_normalization/moving_mean:0', 'anonymous_net_block/batch_normalization/moving_variance:0', 'anonymous_net_block_1/batch_normalization_1/moving_mean:0', 'anonymous_net_block_1/batch_normalization_1/moving_variance:0', 'anonymous_net_block_2/batch_normalization_2/moving_mean:0', 'anonymous_net_block_2/batch_normalization_2/moving_variance:0', 'anonymous_net_block_3/batch_normalization_3/moving_mean:0', 'anonymous_net_block_3/batch_normalization_3/moving_variance:0', 'anonymous_net_block_4/batch_normalization_4/moving_mean:0', 'anonymous_net_block_4/batch_normalization_4/moving_variance:0', 'anonymous_net_block_5/batch_normalization_5/moving_mean:0', 'anonymous_net_block_5/batch_normalization_5/moving_variance:0', 'anonymous_net_block_6/batch_normalization_6/moving_mean:0', 'anonymous_net_block_6/batch_normalization_6/moving_variance:0', 'anonymous_net_block_7/batch_normalization_7/moving_mean:0', 'anonymous_net_block_7/batch_normalization_7/moving_variance:0', 'anonymous_net_block_8/batch_normalization_8/moving_mean:0', 'anonymous_net_block_8/batch_normalization_8/moving_variance:0', 'anonymous_net_block_9/batch_normalization_9/moving_mean:0', 'anonymous_net_block_9/batch_normalization_9/moving_variance:0', 'anonymous_net_block_10/batch_normalization_10/moving_mean:0', 'anonymous_net_block_10/batch_normalization_10/moving_variance:0', 'anonymous_net_block_11/batch_normalization_11/moving_mean:0', 'anonymous_net_block_11/batch_normalization_11/moving_variance:0', 'anonymous_net_block_12/batch_normalization_12/moving_mean:0', 'anonymous_net_block_12/batch_normalization_12/moving_variance:0', 'anonymous_net_block_13/batch_normalization_13/moving_mean:0', 'anonymous_net_block_13/batch_normalization_13/moving_variance:0', 'anonymous_net_block_14/batch_normalization_14/moving_mean:0', 'anonymous_net_block_14/batch_normalization_14/moving_variance:0', 'anonymous_net_block_15/batch_normalization_15/moving_mean:0', 'anonymous_net_block_15/batch_normalization_15/moving_variance:0', 'anonymous_net_block_16/batch_normalization_16/moving_mean:0', 'anonymous_net_block_16/batch_normalization_16/moving_variance:0', 'anonymous_net_block_17/batch_normalization_17/moving_mean:0', 'anonymous_net_block_17/batch_normalization_17/moving_variance:0', 'anonymous_net_block_18/batch_normalization_18/moving_mean:0', 'anonymous_net_block_18/batch_normalization_18/moving_variance:0', 'anonymous_net_block_19/batch_normalization_19/moving_mean:0', 'anonymous_net_block_19/batch_normalization_19/moving_variance:0', 'anonymous_net_block_20/batch_normalization_20/moving_mean:0', 'anonymous_net_block_20/batch_normalization_20/moving_variance:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.421523094177246, Accuracy: 3.13%, Test Loss: 5.430489540100098, Test Accuracy: 2.85% \n",
      "[Training] Epoch 2, Batch 754/1664, Loss: 4.974475860595703, Accuracy: 4.64%  \r"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_on_batch(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_batch)\n",
    "        # tf.print(y_pred.shape)\n",
    "        loss = loss_object(y_true=y_batch, y_pred=y_pred)\n",
    "        # print(y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(y_batch, y_pred)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_on_batch(X_batch, y_batch):\n",
    "    y_pred = model(X_batch)\n",
    "    t_loss = loss_object(y_batch, y_pred)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y_batch, y_pred)\n",
    "    return t_loss\n",
    "    \n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "#     # Reload Data\n",
    "#     if (epoch+1) % reload_data_step == 0:\n",
    "#         X_train = load_data_from_path(X_path_train)\n",
    "        \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    for batch_index, (X_batch, y_batch) in enumerate(batch_loader(X_train, y_train, batch_size=batch_size)):\n",
    "        X_batch = np.divide(np.array(seq(images=X_batch)), 255, dtype=np.float32)  # done by the library\n",
    "#         for i in range(len(X_batch)):\n",
    "#             plt.imshow(X_batch[i])\n",
    "#             plt.title('Real: {}'.format(name_list[y_batch[i]]))\n",
    "#             plt.show()\n",
    "        loss = train_on_batch(X_batch, y_batch)\n",
    "        template = '[Training] Epoch {}, Batch {}/{}, Loss: {}, Accuracy: {:.2%} '\n",
    "        print(template.format(epoch+1,\n",
    "                              batch_index,\n",
    "                              train_size // batch_size,\n",
    "                              loss,\n",
    "                              train_accuracy.result()),\n",
    "             end='\\r')\n",
    "\n",
    "    for batch_index, (X_batch, y_batch) in enumerate(batch_loader(X_dev, y_dev, batch_size=batch_size)):\n",
    "        X_batch = np.divide(X_batch, 255, dtype=np.float32) \n",
    "        loss = test_on_batch(X_batch, y_batch)\n",
    "        template = '[Testing] Epoch {}, Batch {}/{}, Loss: {}, Accuracy: {:.2%} '\n",
    "        print(template.format(epoch+1,\n",
    "                              batch_index,\n",
    "                              dev_size // batch_size,\n",
    "                              loss,\n",
    "                              train_accuracy.result()),\n",
    "             end='\\r')\n",
    "        \n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {:.2%}, Test Loss: {}, Test Accuracy: {:.2%} '\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result(),\n",
    "                         test_loss.result(),\n",
    "                         test_accuracy.result()))\n",
    "    \n",
    "    model.save(CONFIG.model.model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_path_test, y_test = load_data(data_type='test')\n",
    "X_test = load_data_from_path(X_path_test, shape=(CONFIG.image.W, CONFIG.image.H), part=-1)\n",
    "test_size = len(X_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy.reset_states()\n",
    "for batch_index, (X_batch, y_batch) in enumerate(batch_loader(X_test, y_test, batch_size=batch_size)):\n",
    "    X_batch = np.divide(X_batch, 255, dtype=np.float32)\n",
    "    y_pred = model(X_batch)\n",
    "    test_accuracy(y_batch, y_pred)\n",
    "    y_pred_label = np.array([np.argmax(one_hot) for one_hot in y_pred.numpy()])\n",
    "    template = '[Testing] Batch {}/{}, Accuracy: {:.2%} '\n",
    "    print(template.format(batch_index,\n",
    "                          test_size // batch_size,\n",
    "                          test_accuracy.result()),\n",
    "             end='\\r')\n",
    "    for i in range(X_batch.shape[0]):\n",
    "        plt.imshow(X_batch[i])\n",
    "        plt.title('Real: {}, Predict: {}'.format(name_list[y_batch[i]], name_list[y_pred_label[i]]))\n",
    "        plt.show()\n",
    "        \n",
    "template = 'Test Accuracy: {:.2%} '\n",
    "print(template.format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37664bittf2condad2284dedc2eb448ea8250faba7f9d846"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
